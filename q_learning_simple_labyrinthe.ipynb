{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acee5e3e",
   "metadata": {},
   "source": [
    "\n",
    "# Apprentissage par renforcement avec Q-learning\n",
    "\n",
    "Ce notebook montre comment un agent peut apprendre à se déplacer dans une grille pour atteindre un objectif, en utilisant l'algorithme de Q-learning. Il inclut une visualisation du chemin optimal appris.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6256f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paramètres de la grille\n",
    "nb_lignes = 4\n",
    "nb_colonnes = 4\n",
    "etat_final = (3, 3)\n",
    "\n",
    "# Actions possibles\n",
    "actions = ['haut', 'bas', 'gauche', 'droite']\n",
    "action_index = {a: i for i, a in enumerate(actions)}\n",
    "\n",
    "# Q-table initialisée à zéro\n",
    "Q = np.zeros((nb_lignes, nb_colonnes, len(actions)))\n",
    "\n",
    "# Paramètres Q-learning\n",
    "alpha = 0.1    # taux d'apprentissage\n",
    "gamma = 0.9    # facteur de réduction\n",
    "epsilon = 0.2  # exploration (20%)\n",
    "\n",
    "# Fonction pour choisir une action (exploration vs exploitation)\n",
    "def choisir_action(etat):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        return actions[np.argmax(Q[etat[0], etat[1]])]\n",
    "\n",
    "# Fonction pour faire une action et retourner le nouvel état + récompense\n",
    "def faire_action(etat, action):\n",
    "    x, y = etat\n",
    "    if action == 'haut':\n",
    "        x = max(0, x - 1)\n",
    "    elif action == 'bas':\n",
    "        x = min(nb_lignes - 1, x + 1)\n",
    "    elif action == 'gauche':\n",
    "        y = max(0, y - 1)\n",
    "    elif action == 'droite':\n",
    "        y = min(nb_colonnes - 1, y + 1)\n",
    "\n",
    "    nouvel_etat = (x, y)\n",
    "    if nouvel_etat == etat_final:\n",
    "        return nouvel_etat, 10  # Récompense finale\n",
    "    else:\n",
    "        return nouvel_etat, -1  # Pénalité pour mouvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdb06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entraînement Q-learning\n",
    "nb_episodes = 500\n",
    "\n",
    "for episode in range(nb_episodes):\n",
    "    etat = (0, 0)  # Départ\n",
    "    while etat != etat_final:\n",
    "        action = choisir_action(etat)\n",
    "        nouvel_etat, recompense = faire_action(etat, action)\n",
    "        a_index = action_index[action]\n",
    "\n",
    "        Q[etat[0], etat[1], a_index] += alpha * (\n",
    "            recompense + gamma * np.max(Q[nouvel_etat[0], nouvel_etat[1]]) - Q[etat[0], etat[1], a_index]\n",
    "        )\n",
    "        etat = nouvel_etat\n",
    "\n",
    "# Affichage de la Q-table\n",
    "print(\"Q-table après entraînement :\")\n",
    "for i in range(nb_lignes):\n",
    "    for j in range(nb_colonnes):\n",
    "        print(f\"État ({i},{j}) : {Q[i, j]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cce547",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Générer le chemin optimal à partir de (0,0)\n",
    "def chemin_optimal():\n",
    "    etat = (0, 0)\n",
    "    chemin = [etat]\n",
    "    max_etapes = 20  # éviter les boucles infinies\n",
    "    for _ in range(max_etapes):\n",
    "        if etat == etat_final:\n",
    "            break\n",
    "        action = actions[np.argmax(Q[etat[0], etat[1]])]\n",
    "        etat, _ = faire_action(etat, action)\n",
    "        chemin.append(etat)\n",
    "    return chemin\n",
    "\n",
    "chemin = chemin_optimal()\n",
    "print(\"\\nChemin optimal trouvé :\")\n",
    "for e in chemin:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualisation du chemin\n",
    "def afficher_grille(chemin):\n",
    "    grille = np.zeros((nb_lignes, nb_colonnes))\n",
    "    for idx, (x, y) in enumerate(chemin):\n",
    "        grille[x, y] = idx + 1  # marquer les étapes\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(grille, cmap='Blues', origin='upper')\n",
    "\n",
    "    for i in range(nb_lignes):\n",
    "        for j in range(nb_colonnes):\n",
    "            txt = 'G' if (i,j)==etat_final else int(grille[i,j]) if grille[i,j]>0 else ''\n",
    "            plt.text(j, i, txt, ha='center', va='center', fontsize=12, color='black')\n",
    "\n",
    "    plt.title(\"Chemin optimal appris\")\n",
    "    plt.xticks(np.arange(nb_colonnes))\n",
    "    plt.yticks(np.arange(nb_lignes))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "afficher_grille(chemin)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}